# NimbusSync â˜ï¸ğŸ“¦

**NimbusSync** is a modular, Dockerized data orchestration platform designed to sync and process Amazon SP-API data with Airflow. The name **Nimbus** refers to a cloud, aligning with modern cloud-based data infrastructures, and **Sync** captures the projectâ€™s core functionality: synchronizing e-commerce data reliably and repeatedly.

---

## ğŸ“Œ Project Objective

To create a robust, modular, and extensible platform that:
- Pulls and stores **Amazon SP-API data** (orders, order items, summaries)
- Schedules and automates these sync jobs using **Apache Airflow**
- Logs operations and errors for easy monitoring
- Uses Docker for environment consistency and portability
- Is production-ready and GitHub-presentable

---

## ğŸ“ Folder Structure

```
NimbusSync/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                  # Airflow DAGs for scheduling sync
â”‚   â”œâ”€â”€ logs/                  # Airflow execution logs
â”‚   â””â”€â”€ plugins/               # Custom Airflow plugins (if any)
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ connect.py             # SQLAlchemy DB engine setup
â”‚   â””â”€â”€ models.py              # PostgreSQL table definitions
â”œâ”€â”€ sales_data/
â”‚   â”œâ”€â”€ amazon_sync.py         # Full historical Amazon order sync
â”‚   â”œâ”€â”€ order_items_sync.py    # Sync order item-level data
â”‚   â”œâ”€â”€ fetch_historic_sales.py
â”‚   â””â”€â”€ update_daily_sales.py  # Daily incremental update script
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py              # Logger setup
â”‚   â””â”€â”€ monitor.py             # Post-sync data validation checker
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ errors-and-fixes.md    # All problems faced and how we solved them
â”‚   â””â”€â”€ command-cheatsheet.md  # Docker, Airflow, and Python CLI command reference
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Dockerfile.airflow
â”œâ”€â”€ README.md
â””â”€â”€ run_all.py
```

---

## âš™ï¸ Technologies Used

- **Python 3.9** for all sync logic and SP-API integration
- **Amazon Selling Partner API (SP-API)** for data extraction
- **PostgreSQL** for data persistence
- **SQLAlchemy ORM** for DB interaction
- **Apache Airflow 2.7.2** for orchestration
- **Docker** for reproducibility across machines

---
## ğŸ§ª How to Run Locally

```bash
# 1. Clone repo & move in
git clone <repo_url>
cd amazon-sync-engine

# 2. Start Docker containers
docker-compose up --build -d

# 3. Access Airflow UI
http://localhost:8080 (username: airflow, password: airflow)

# 4. Enable + trigger DAGs
```
## âœ… Progress and Phases

| Phase                  | Goal | Status |
|------------------------|------|--------|
| 1. Local MVP           | Docker + PostgreSQL + Sync scripts working | âœ… Done |
| 2. Airflow Integration | DAGs setup and working in UI | âœ… Done |
| 3. Retry + Monitoring  | Add retry + monitor failed inserts | â¬œ Pending |
| 4. Forecasting         | Add Prophet to forecast sales | â¬œ Planned |
| 5. Dashboarding        | Streamlit or FastAPI dashboard | â¬œ Planned |
| 6. Cloud Deployment    | Host with Cloud Run + Cloud SQL | â¬œ Planned |

---

## ğŸ” Author & Credits

Made with â¤ï¸ by **Surya** â€” built from scratch with strong attention to modularity, education, and documentation.

---

For full error logs and how issues were fixed, see [`docs/errors-and-fixes.md`](docs/errors-and-fixes.md)  
For Docker/Airflow/Python commands, see [`docs/command-cheatsheet.md`](docs/command-cheatsheet.md)
